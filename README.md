<<<<<<< HEAD
ðŸš€ Atla AI Agent

Serverless + Containerized AI Backend | RAG | Persistent Memory | Multi-Cloud Deployment

Atla AI Agent â€” atla.in

A production-grade conversational AI agent with persistent memory, Retrieval-Augmented Generation (RAG), and a modern glassmorphism UI.

Live demo: https://atla.in

This project demonstrates:

FastAPI-based AI backend

Persistent vector memory (ChromaDB)

Claude API (production) + Ollama (local dev)

Dual deployment: Vercel (serverless) + Railway (container runtime)

Git-based CI/CD automation

âœ¨ What It Does

Answers questions using a local knowledge base (RAG via ChromaDB)

Remembers conversations across server restarts

Routes to Claude API in production

Supports Ollama + Llama 3 locally

Clean animated dark UI with real-time â€œThinkingâ€¦â€ state

Deployable across multiple cloud runtimes from a single repo

ðŸ— Architecture Overview
AI_Agent_1/
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ index.py          # Primary FastAPI application (Vercel runtime)
â”‚
â”œâ”€â”€ app1.py               # Railway entrypoint (imports api.index)
â”œâ”€â”€ ingest.py             # Ingest single file into vector store
â”œâ”€â”€ ingest_all.py         # Bulk ingest knowledge base
â”œâ”€â”€ memory_test.py        # ChromaDB smoke test
â”‚
â”œâ”€â”€ knowledge_base/       # Drop .txt files here
â”œâ”€â”€ chroma_db/            # Persistent vector database (gitignored)
â”‚
â”œâ”€â”€ index.html            # Frontend UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ vercel.json           # Vercel routing config
â””â”€â”€ README.md
â˜ï¸ Deployment Model
Platform	Role
Vercel	Static frontend + serverless backend
Railway	Persistent container backend
Netlify (optional)	Static frontend pointing to Railway
How It Works

api/index.py â†’ primary FastAPI app

app1.py â†’ thin wrapper for Railway (from api.index import app)

Same backend logic deployed to two environments

Push to Git â†’ auto redeploy on both platforms

ðŸ§  Tech Stack
Layer	Technology
Backend	FastAPI + Uvicorn
Vector DB	ChromaDB (persistent)
LLM (prod)	Claude API (claude-3-5-haiku)
LLM (dev)	Ollama + Llama 3
Frontend	Vanilla HTML / CSS / JS
Serverless	Vercel
Container Runtime	Railway
ðŸ’» Quick Start (Local)
1ï¸âƒ£ Clone & Setup
=======
# ðŸš€ Atla AI Agent

Production-grade conversational AI system with persistent memory, Retrieval-Augmented Generation (RAG), and dual-cloud deployment.

Live: https://atla.in


## Overview

Atla AI Agent is a FastAPI-based AI backend with persistent vector memory using ChromaDB.  
It supports Claude (production) and Ollama (local development), and is deployed across:

- Vercel (serverless runtime)
- Railway (container runtime)

Single repository. Multi-platform deployment. Automated CI/CD.


## Architecture

AI_Agent_1/
â”‚
â”œâ”€â”€ api/index.py        # Primary FastAPI app (Vercel runtime)
â”œâ”€â”€ app1.py             # Railway entrypoint (imports api.index)
â”œâ”€â”€ ingest.py           # Single file ingestion
â”œâ”€â”€ ingest_all.py       # Bulk ingestion
â”œâ”€â”€ memory_test.py      # Chroma smoke test
â”œâ”€â”€ knowledge_base/     # Drop .txt files here
â”œâ”€â”€ chroma_db/          # Persistent vector store (gitignored)
â”œâ”€â”€ index.html          # Frontend UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ vercel.json
â””â”€â”€ README.md


## Core Features

- Retrieval-Augmented Generation (RAG)
- Persistent ChromaDB memory
- Claude API integration
- Ollama + Llama 3 local mode
- Conversation compression
- Multi-cloud deployment
- Git-based CI/CD


## Local Development

Clone and setup:

>>>>>>> 9777fa6 (Rewrite README cleanly)
git clone https://github.com/saipranavAtla/atla-agent.git
cd atla-agent
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
<<<<<<< HEAD
2ï¸âƒ£ Optional: Run with Ollama (Local Dev)

In a separate terminal:

ollama serve
ollama pull llama3
3ï¸âƒ£ Optional: Use Claude API
export ANTHROPIC_API_KEY=sk-ant-...
4ï¸âƒ£ Ingest Knowledge Base
python ingest_all.py

This creates the persistent chroma_db/ directory.

5ï¸âƒ£ Start Backend
uvicorn app1:app --reload --port 8000
6ï¸âƒ£ Open UI

Either open index.html directly
or serve it:

python -m http.server 3000

Visit:

http://localhost:3000
ðŸ“¡ API Endpoints
Method	Endpoint	Description
POST	/generate/	Chat with the AI agent
GET	/health	Backend health check
GET	/	Root info
Example Request
curl -X POST http://localhost:8000/generate/ \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What services do you offer?"}'
ðŸ“š Adding Knowledge

Drop any .txt file into:

knowledge_base/

Then run:

python ingest_all.py

The AI will incorporate the new content into retrieval responses.

ðŸš‚ Deploy to Railway

Push repo to GitHub

Go to railway.app â†’ New Project â†’ Deploy from GitHub

Set environment variable:

ANTHROPIC_API_KEY = sk-ant-...

Start command:

uvicorn app1:app --host 0.0.0.0 --port $PORT

Railway auto deploys

Update frontend API_BASE if using Railway URL

Point DNS (atla.in) to Railway

ðŸš€ Deploy to Vercel

Connect GitHub repo to Vercel

Ensure vercel.json exists

Set required environment variables

Push to main

Auto deployment triggers

Vercel handles:

Static frontend hosting

Serverless FastAPI backend via /api/index.py

ðŸ” Environment Variables
Variable	Required	Description
ANTHROPIC_API_KEY	Yes (Claude mode)	Production LLM
OPENAI_API_KEY	Optional	If using OpenAI mode
OLLAMA_BASE_URL	Optional	Local dev mode

Must be configured separately per platform.

âš™ï¸ CI/CD Workflow

Every git push:

Vercel rebuilds frontend + serverless backend

Railway redeploys backend

No manual build steps required

âš  Limitations

Current architecture is suitable for:

Personal AI agents

Small-scale SaaS

Portfolio projects

Experimental deployments

For enterprise scale:

Replace in-memory rate limiting with Redis

Add distributed caching

Add structured logging

Add observability (Prometheus / OpenTelemetry)

Add horizontal scaling strategy

ðŸ›  Future Roadmap

Streaming responses

Advanced memory layering

Redis-backed distributed rate limiting

Vector DB cloud migration

Custom domain API subdomain (api.atla.in)

Structured logging & monitoring

ðŸ‘¤ Built By

Sai Pranav Atla
AI Engineer â€” Bengaluru

LinkedIn

GitHub

https://atla.in

ðŸ”¥ What This Project Demonstrates

Retrieval-Augmented Generation (RAG)

Persistent vector memory

Dual runtime deployment (serverless + container)

Clean Git-based CI/CD

Multi-cloud architecture from one codebase

This repository is both a functional AI system and an infrastructure learning project.
=======


Run with Claude:

export ANTHROPIC_API_KEY=sk-ant-...


Optional: Run Ollama locally

ollama serve
ollama pull llama3


Ingest knowledge:

python ingest_all.py


Start server:

uvicorn app1:app --reload --port 8000


Serve frontend:

python -m http.server 3000

Open:
http://localhost:3000


## API Endpoints

POST  /generate/   â†’ Chat with AI  
GET   /health      â†’ Backend status  
GET   /            â†’ Root info  


Example:

curl -X POST http://localhost:8000/generate/ \
  -H "Content-Type: application/json" \
  -d '{"prompt":"What services do you offer?"}'


## Deployment

Railway:
- Deploy from GitHub
- Set ANTHROPIC_API_KEY
- Start command:
  uvicorn app1:app --host 0.0.0.0 --port $PORT

Vercel:
- Connect GitHub
- Ensure vercel.json exists
- Set environment variables
- Push to main â†’ auto deploy


## Environment Variables

ANTHROPIC_API_KEY  (Production LLM)
OPENAI_API_KEY     (Optional)
OLLAMA_BASE_URL    (Local dev)


## Built By

Sai Pranav Atla
AI Engineer â€” Bengaluru
https://atla.in
>>>>>>> 9777fa6 (Rewrite README cleanly)
